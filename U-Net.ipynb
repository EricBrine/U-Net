{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3 CSC420.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_SXBKg49i4z",
        "colab_type": "code",
        "outputId": "59e315e9-dd10-47c6-d077-31e89d1f57f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2vwVZlct6J6",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9g4E-7xt2to",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import glob\n",
        "import re\n",
        "import cv2\n",
        "from sys import stdout\n",
        "from itertools import islice\n",
        "from tqdm import tqdm\n",
        "#from model import UNet\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.transforms import ToPILImage, ToTensor, Normalize, Compose\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.ndimage import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import copy\n",
        "import math\n",
        "import sys\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE1ZGz0GuCfp",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDwFxEwzB9qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "  def __init__(self, transfer_learning=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.initial_conv = down_block(3, 64, False) \n",
        "    self.down1 = down_block(64, 128, True) \n",
        "    self.down2 = down_block(128, 256, True) \n",
        "    self.down3 = down_block(256, 512, True) \n",
        "    self.down4 = down_block(512, 1024, True) \n",
        "\n",
        "    self.up1 = up_block(512)\n",
        "    self.up2 = up_block(256)\n",
        "    self.up3 = up_block(128)\n",
        "    self.up4 = up_block(64)\n",
        "    self.out = nn.Conv2d(64, 2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.x1 = self.initial_conv(x)\n",
        "    self.x2 = self.down1(self.x1)\n",
        "    self.x3 = self.down2(self.x2)\n",
        "    self.x4 = self.down3(self.x3)\n",
        "    self.x5 = self.down4(self.x4)\n",
        "\n",
        "    x = self.up1(self.x5, self.x4)\n",
        "    x = self.up2(x, self.x3)\n",
        "    x = self.up3(x, self.x2)\n",
        "    x = self.up4(x, self.x1)\n",
        "\n",
        "    return F.softmax(self.out(x), dim=1)\n",
        "\n",
        "class down_block(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, downsample):\n",
        "    super().__init__()\n",
        "    self.double_conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "    self.downsample = downsample\n",
        "    self.max_pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "  def enable(self):\n",
        "    for name,param in self.named_parameters():\n",
        "      param.requires_grad = True\n",
        "      param.requires_grad = True\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.downsample == True:\n",
        "      x = self.max_pool(x)\n",
        "    x = self.double_conv(x)\n",
        "    return x\n",
        "\n",
        "class up_block(nn.Module):\n",
        "\n",
        "  def __init__(self, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.up_sampling = nn.ConvTranspose2d(out_channels*2, out_channels, 2, stride=2)\n",
        "\n",
        "    self.double_conv = nn.Sequential(\n",
        "      nn.Conv2d(out_channels*2, out_channels, 3, padding=1),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "  def enable(self):\n",
        "    for name,param in self.named_parameters():\n",
        "      param.requires_grad = True\n",
        "      param.requires_grad = True\n",
        "\n",
        "  def forward(self, x, prev_x):\n",
        "    x = self.up_sampling(x)\n",
        "    x = torch.cat((x, prev_x), dim=1)\n",
        "    x = self.double_conv(x)\n",
        "    return x\n",
        "\n",
        "def weights_init(m):\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):    \n",
        "    m.weight.data.normal_(0, math.sqrt(2/(m.in_channels*9)))\n",
        "    m.bias.data.normal_(0, math.sqrt(2/(m.in_channels*9)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wMxXMy7t-VM",
        "colab_type": "text"
      },
      "source": [
        "Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJjGroq0uAnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VerticalFlip:\n",
        "  def __init__(self, probability=0.3):\n",
        "    self.probability = probability\n",
        "  def __call__(self, images):\n",
        "    img = images[0]\n",
        "    mask = images[1]\n",
        "    if random.random() < self.probability:\n",
        "      img = cv2.flip(img, 0)\n",
        "      mask = cv2.flip(mask, 0).reshape(128,128,1)\n",
        "    return img, mask\n",
        "\n",
        "class HorizontalFlip:\n",
        "  def __init__(self, probability=0.3):\n",
        "    self.probability = probability\n",
        "  def __call__(self, images):\n",
        "    img = images[0]\n",
        "    mask = images[1]\n",
        "    if random.random() < self.probability:\n",
        "      img = cv2.flip(img, 1)\n",
        "      mask = cv2.flip(mask, 1).reshape(128,128,1)\n",
        "    return img, mask\n",
        "  \n",
        "class Rotate:\n",
        "  def __init__(self, probability=0.15):\n",
        "    self.probability = probability\n",
        "  def __call__(self, images):\n",
        "    img = images[0]\n",
        "    mask = images[1]\n",
        "    random_num = random.random()\n",
        "    if random_num < self.probability/3:\n",
        "      \n",
        "      M = cv2.getRotationMatrix2D((img.shape[1]/2,img.shape[0]/2), 90, 1.)\n",
        "      img = cv2.warpAffine(img, M, (img.shape[1],img.shape[0]))\n",
        "     \n",
        "      mask[:,:,0] = np.rot90(mask[:,:,0], k=1, axes=(1,0))\n",
        "      \n",
        "    elif random_num < (2*self.probability)/3:\n",
        "      \n",
        "      M = cv2.getRotationMatrix2D((img.shape[1]/2,img.shape[0]/2), 180, 1.)\n",
        "      img = cv2.warpAffine(img, M, (img.shape[1],img.shape[0]))\n",
        "     \n",
        "      mask[:,:,0] = np.rot90(mask[:,:,0], k=2, axes=(1,0))\n",
        "      \n",
        "    elif random_num < self.probability:\n",
        "      \n",
        "      M = cv2.getRotationMatrix2D((img.shape[1]/2,img.shape[0]/2), 180, 1.)\n",
        "      img = cv2.warpAffine(img, M, (img.shape[1],img.shape[0]))\n",
        "     \n",
        "      mask[:,:,0] = np.rot90(mask[:,:,0], k=3, axes=(1,0))\n",
        "       \n",
        "    return img, mask\n",
        "\n",
        "class Zoom:\n",
        "  def __init__(self, probability=0.1):\n",
        "    self.probability = probability\n",
        "  def __call__(self, images):\n",
        "    img = images[0]\n",
        "    mask = images[1]\n",
        "    if random.random() < self.probability:\n",
        "      zoom_magnitude = 1 + self.probability\n",
        "      img = zoom(img, (zoom_magnitude,zoom_magnitude,1))\n",
        "      mask = zoom(img, (zoom_magnitude,zoom_magnitude, 1))\n",
        "      w, h = img.shape[1], img.shape[0]\n",
        "      a, b = int(w/2 - (64)), int(h/2 + (64))\n",
        "      img = img[a:b,a:b,:]\n",
        "      mask = mask[a:b,a:b,0]      \n",
        "      \n",
        "    return img, mask\n",
        "\n",
        "train_transform = Compose([\n",
        "  VerticalFlip(),\n",
        "  HorizontalFlip(),\n",
        "  Rotate(),\n",
        "  Zoom()\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCMrx22DuF9R",
        "colab_type": "text"
      },
      "source": [
        "Training and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2lpomxA39UY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## Alter these as required for testing. \n",
        "## You can access the CSC420_A3 drive using the shareable link\n",
        "## https://drive.google.com/drive/folders/1mt-_YWULaaFSe8WqWVfiiwpc4Yldmewg?usp=sharing\n",
        "train_inputs_path = \"drive/My Drive/CSC420_A3/cat_data/Train/input/*.jpg\"\n",
        "train_masks_path = \"drive/My Drive/CSC420_A3/cat_data/Train/mask/*.jpg\"\n",
        "\n",
        "transfer_train_inputs_path = \"drive/My Drive/CSC420_A3/extra_data/input/*.jpg\"\n",
        "transfer_train_masks_path = \"drive/My Drive/CSC420_A3/extra_data/mask/*.png\"\n",
        "\n",
        "test_inputs_path = \"drive/My Drive/CSC420_A3/cat_data/Test/input/*.jpg\"\n",
        "test_masks_path = \"drive/My Drive/CSC420_A3/cat_data/Test/mask/*.jpg\"\n",
        "\n",
        "def dice(expected, x):\n",
        "  num_classes = 2\n",
        "  dims = (0,2) \n",
        "  s=1e-7\n",
        "  one_hot_expected = torch.eye(num_classes)[expected.squeeze(1)]\n",
        "  one_hot_expected = one_hot_expected.permute(0, 3, 1, 2).float()\n",
        "  \n",
        "  numerator = 2. * (torch.sum(x * one_hot_expected, (0,2,3)))\n",
        "  denominator = torch.sum(x + one_hot_expected, (0,2,3)) + s  \n",
        "  dice_loss = (numerator / denominator).mean()\n",
        "  return (dice_loss)\n",
        "\n",
        "def dice_loss(expected, x):\n",
        "  return (1 - dice(expected, x))\n",
        "\n",
        "def digits_in_string(text):\n",
        "\tconvert = lambda text: int(text) if text.isdigit() else text.lower() \n",
        "\treturn [ convert(c) for c in re.split(r'(\\d+)', text) ]\n",
        "\n",
        "def read_data(inputs, masks, augment_data = False, transfer_learning=False):\n",
        "\n",
        "  img_filenames = glob.glob(inputs)\n",
        "  img_filenames.sort(key=digits_in_string)\n",
        "  mask_filenames = glob.glob(masks)\n",
        "  mask_filenames.sort(key=digits_in_string)\n",
        "  \n",
        "  train_data = []\n",
        "  images = []\n",
        "  masks = []\n",
        "  # Read and resize and reshape the images and their corresponding masks\n",
        "  # Append each image and mask to train_data\n",
        "  for i in range(len(img_filenames)):\n",
        "    img = cv2.imread(img_filenames[i])\n",
        "    img = (cv2.resize(img,(128,128)))/255\n",
        "    mask = cv2.imread(mask_filenames[i], cv2.IMREAD_GRAYSCALE)\n",
        "    mask = ((cv2.resize(mask,(128,128)))).reshape(128,128,1)\n",
        "    # The masks for use in transfer learning are already normalized\n",
        "    if not transfer_learning:\n",
        "      mask = mask/255\n",
        "    mask = np.where(mask > 0.5, 1, 0)\n",
        "    images.append(copy.deepcopy(img))\n",
        "    masks.append(copy.deepcopy(mask))\n",
        "    transform = transforms.ToTensor()\n",
        "    train_data.append([transform(img).float(), transform(mask).long()])\n",
        "  \n",
        "  # If the data is augmented, it is doubled in size and half the images\n",
        "  # undergo random transformations.\n",
        "  if augment_data: \n",
        "    images, masks = images, masks\n",
        "    for i in range(len(images)):\n",
        "            \n",
        "      img, mask = images[i], masks[i]      \n",
        "      img, mask = train_transform([img, mask])\n",
        "      transform = transforms.ToTensor()\n",
        "      img, mask = transform(img), transform(mask)\n",
        "      train_data.append([img.float(), mask.long()])\n",
        "    \n",
        "  return train_data\n",
        "    \n",
        "def train(train_data, model=None, num_epochs=20, batch_size=1, lr=0.02, transfer_learning=False):\n",
        "  \n",
        "  if model == None:\n",
        "    model = UNet()\n",
        "    model.train()\n",
        "    # Initialize weights\n",
        "    model.apply(weights_init)\n",
        "  elif model != None and transfer_learning:\n",
        "    model = UNet()\n",
        "    model.train()\n",
        "    # Freeze all layers\n",
        "    for name,param in model.named_parameters():\n",
        "      param.requires_grad = False\n",
        "    # Unfreeze the last layers\n",
        "    model.up4.enable()\n",
        "    model.up3.enable()\n",
        "    model.up2.enable()\n",
        "    model.up1.enable()\n",
        "    # Reset the weights of the last layers\n",
        "    model.out.apply(weights_init)\n",
        "    model.up4.apply(weights_init)\n",
        "    model.up3.apply(weights_init)\n",
        "    model.up2.apply(weights_init)\n",
        "    model.out.requires_grad = True\n",
        "    model.out.weight.requires_grad = True\n",
        "    model.out.bias.requires_grad = True\n",
        "    \n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Epoch: \", epoch+1)\n",
        "    running_loss = 0\n",
        "    for i, data in enumerate(train_loader):  \n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      x, real_mask = data[0], data[1][:,0,:,:]\n",
        "      x.requires_grad = True      \n",
        "      if torch.cuda.is_available() and model.cuda():\n",
        "        x = x.cuda()\n",
        "        real_mask = real_mask.cuda()\n",
        "\n",
        "      output = model(x.float())\n",
        "\n",
        "      #### Calculate loss\n",
        "      loss = dice_loss(real_mask, output)\n",
        "      # loss = F.cross_entropy(output.float(), real_mask.long())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      running_loss += loss.item()\n",
        "      \n",
        "    print(f\"Training loss: {running_loss/len(train_loader)}\")\n",
        "    \n",
        "  return model\n",
        "\n",
        "def test(test_data, model):\n",
        "  model.eval()\n",
        "  test_loader = DataLoader(test_data, shuffle=False, batch_size=1)\n",
        "  running_loss = 0\n",
        "  accuracy = 0\n",
        "  stop = 0\n",
        "  for i, data in enumerate(test_loader):\n",
        "    \n",
        "    x, y_expected = data[0], data[1][0,0,:,:]\n",
        "    \n",
        "    if torch.cuda.is_available() and model.cuda():\n",
        "        x = x.cuda()\n",
        "        y_expected = y_expected.cuda()\n",
        "    \n",
        "    output = model(x.float())\n",
        "\n",
        "    # Get final cat segmentation image\n",
        "    _, predicted = torch.max(output.data, 1, keepdim=True)\n",
        "\n",
        "    # Sorensen-dice coefficient for only the final product, rather than\n",
        "    # the two segment dice score computed during training\n",
        "    intersection = torch.sum(predicted[0,0,:,:]*y_expected[:,:])\n",
        "    cardinality = torch.sum(predicted[0,0,:,:] + y_expected[:,:])\n",
        "    score = (2. * intersection) / (cardinality + 1e-7)\n",
        "\n",
        "    accuracy += score.item()\n",
        "  print(f\"\\n\\nTest Accuracy: {accuracy/len(test_loader)}\")\n",
        "\n",
        "\n",
        "def visualize_segmentation(model, test_data, samples=10):\n",
        "  model.eval()\n",
        "  test_loader = DataLoader(test_data, shuffle=True, batch_size=1)\n",
        "  running_loss = 0\n",
        "  accuracy = 0\n",
        "  stop = 0\n",
        "  for i, data in enumerate(test_loader):\n",
        "    if i == samples:\n",
        "      break\n",
        "    x, y_expected = data[0], data[1][:,0,:,:]\n",
        "    if torch.cuda.is_available() and model.cuda():\n",
        "      x = x.cuda()\n",
        "      y_expected = y_expected.cuda()\n",
        "    output = model(x.float())\n",
        "    _, predicted = torch.max(output.data, 1, keepdim=True)\n",
        "    predicted = predicted[0,0,:,:].detach().cpu().numpy()\n",
        "\n",
        "    # Find the contours of the mask\n",
        "    border = cv2.copyMakeBorder(predicted, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0 )\n",
        "    _, contours, _ = cv2.findContours(np.uint8(border), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Sometimes there are other small contours in the image. Make sure its the cat contour. \n",
        "    longest_contour = 0\n",
        "    if len(contours) > 1:\n",
        "      for i in range(1, len(contours)):\n",
        "        if contours[i].shape[0] > contours[longest_contour].shape[0]:\n",
        "          longest_contour = i\n",
        "      contours = [contours[longest_contour]]\n",
        "    input_img = data[0][0,:,:,:].permute(1,2,0).detach().cpu().numpy()\n",
        "    blank = np.zeros(predicted.shape)\n",
        "    img = cv2.drawContours(blank.copy(), contours, -1, (255, 255, 255), 2)\n",
        "    img = np.stack((img,)*3, axis=-1)\n",
        "    segmented = input_img + img\n",
        "\n",
        "    ## Download the image.\n",
        "    ## Uncomment this if you would like to try downloading segmentation images.\n",
        "    # cv2.imwrite(\"seg\" + str(i) + \".png\", segmented*255)\n",
        "    # files.download(\"seg\" + str(i) + \".png\")\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb_x5rAss6Rg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Main:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5FtjAvxCN0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "torch.manual_seed(1234)\n",
        "if torch.cuda.is_available():\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "  torch.cuda.manual_seed_all(1234)\n",
        "  print('Running on GPU: {}.'.format(torch.cuda.get_device_name()))\n",
        "else:\n",
        "  print('Running on CPU.')\n",
        "\n",
        "# Set True to train the model for 1.1.\n",
        "# If augment_data is set to True it will train it using augmented data for 1.2 \n",
        "train_model = False\n",
        "\n",
        "# Set True to load the model from 1.1.\n",
        "# If augment_data is set to True it will load the model trained with augmented data from 1.2 \n",
        "load_model = False\n",
        "\n",
        "# Train the transfer model which is to be transfered on other data\n",
        "train_transfer_model = False\n",
        "\n",
        "# Take the transfer model and use transfer learning to train the model for cat segmentation\n",
        "transfer_learning_training = False\n",
        "\n",
        "# Load the model which was trained with transfer learning\n",
        "load_model_using_transfer_learning = True \n",
        "\n",
        "test_model = True\n",
        "\n",
        "# Whether to train/load with augmented data for 1.2\n",
        "augment_data = False\n",
        "\n",
        "# Download images of visualized segmentation\n",
        "# Need to uncomment the command to download\n",
        "visualize = False\n",
        "\n",
        "#### 1.1/1.2\n",
        "if train_model:\n",
        "  if not augment_data:\n",
        "    train_data = read_data(train_inputs_path, train_masks_path, augment_data=False)\n",
        "    model = train(train_data, num_epochs=50, lr=0.01, batch_size=8)\n",
        "    # torch.save(model.state_dict(), 'drive/My Drive/model_weights_11.pth')\n",
        "  else:\n",
        "    train_data = read_data(train_inputs_path, train_masks_path, augment_data=True)\n",
        "    model = train(train_data, num_epochs=50, lr=0.01, batch_size=8)\n",
        "    # torch.save(model.state_dict(), 'drive/My Drive/model_weights_augmented_data_12.pth')\n",
        "\n",
        "\n",
        "if load_model:\n",
        "  model = UNet()\n",
        "  if not augment_data:\n",
        "    model.load_state_dict(torch.load('drive/My Drive/model_weights_11.pth'))\n",
        "  else:\n",
        "    model.load_state_dict(torch.load('drive/My Drive/model_weights_augmented_data_12.pth'))\n",
        "\n",
        "\n",
        "#### 3.3 TRANSFER LEARNING\n",
        "# train the model on a larger, similar data set\n",
        "if train_transfer_model:\n",
        "  train_data = read_data(transfer_train_inputs_path, transfer_train_masks_path, augment_data=False, transfer_learning=True)\n",
        "  model = train(train_data, lr=0.1, batch_size=16, num_epochs=30)\n",
        "  torch.save(model.state_dict(), 'drive/My Drive/transfer_model.pth')\n",
        "\n",
        "# load the transfer model and train it on the cat data\n",
        "if transfer_learning_training:\n",
        "  train_data = read_data(train_inputs_path, train_masks_path, augment_data=False)\n",
        "  model = UNet()\n",
        "  model.load_state_dict(torch.load('drive/My Drive/transfer_model.pth'))\n",
        "  model = train(train_data, num_epochs=40, batch_size=4, model=model, transfer_learning=True, lr=0.01)\n",
        "  # torch.save(model.state_dict(), 'drive/My Drive/model_with_transfer_learning_weights.pth')\n",
        "\n",
        "# load the transfer learning model for testing\n",
        "if load_model_using_transfer_learning:\n",
        "  model = UNet()\n",
        "  model.load_state_dict(torch.load('drive/My Drive/model_with_transfer_learning_weights.pth'))\n",
        "\n",
        "if test_model:\n",
        "  test_data = read_data(test_inputs_path, test_masks_path)\n",
        "  test(test_data, model)\n",
        "\n",
        "if visualize:\n",
        "  test_data = read_data(test_inputs_path, test_masks_path)\n",
        "  visualize_segmentation(model, test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}